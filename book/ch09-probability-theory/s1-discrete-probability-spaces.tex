\section{Discrete probability spaces}
\secbegin{secDiscreteProbabilitySpaces}

Probability theory is a field of mathematics which attempts to model randomness and uncertainty in the `real world'. The mathematical machinery it develops allows us to understand how this randomness behaves and to extract information which is useful for making predictions.

\textit{Discrete} probability theory, in particular, concerns situations in which the possible outcomes form a \textit{countable} set. This simplifies matters considerably: if there are only countably many outcomes, then the probability that any event occurs is determined entirely by the probabilities that the individual outcomes comprised by the event occur.

For example, the number $N$ of words spoken by a child over the course of a year takes values in $\mathbb{N}$, so is discrete. To each $n \in \mathbb{N}$, we may assign a probability that $N=n$, which can take positive values in a meaningful way, and from these probabilities we can compute the probabilities of more general events occurring (e.g.\ the probability that the child says under a million words). However, the height $H$ grown by the child over the same period takes values in $[0,\infty)$, which is uncountable; for each $h \in [0,\infty)$, the probability that $H=h$ is zero, so these probabilities give us no information. We must study the behaviour of $H$ through some other means.

In this chapter, we will concern ourselves only with the discrete setting.

It is important to understand from the outset that, although we use language like \textit{outcome}, \textit{event}, \textit{probability} and \textit{random}, and although we use real-world examples, everything we do concerns mathematical objects: sets, elements of sets, and functions. If we say, for example, ``the probability that a roll of a fair six-sided die shows $3$ or $4$ is $\frac{1}{3}$,'' we are actually interpreting the situation mathematically---the \textit{outcomes} of the die rolls are interpreted as the elements of the set $[6]$; the \textit{event} that the die shows $3$ or $4$ is interpreted as the subset $\{ 3, 4 \} \subseteq [6]$; and the \textit{probability} that this event occurs is the value of a particular function $\mathbb{P} : \mathcal{P}([6]) \to [0,1]$ on input $\{ 3, 4 \}$. The mathematical interpretation is called a \textbf{model}\index{model!probablistic} of the real-world situation.

\begin{definition}
\label{defDiscreteProbabilitySpace}
\index{probability space!discrete}
\index{probability}
\index{probability measure!discrete}
\index{sample space}
\index{outcome}
\index{event}
\index{countable additivity}
A \textbf{discrete probability space} is a pair $(\Omega, \mathbb{P})$\nindex{OmegaP}{$(\Omega,\mathbb{P})$}{probability space} \inlatexnb{(\textbackslash{}Omega,\ \textbackslash{}mathbb\{P\})}\lindexmmc{mathbb}{$\mathbb{A}, \mathbb{B}, \dots$}, consisting of a countable set $\Omega$ and a function $\mathbb{P} : \mathcal{P}(\Omega) \to [0,1]$,\nindex{Pr}{$\mathbb{P}$}{probability} such that
\begin{enumerate}[(i)]
\item $\mathbb{P}(\Omega) = 1$; and
\item (\textbf{Countable additivity}) If $\{ A_i \mid i \in I \}$ is any family of pairwise disjoint subsets of $\Omega$, indexed by a countable set $I$, then
\[ \displaystyle \mathbb{P} \left( \bigcup_{i \in I} A_i \right) = \sum_{i \in I} \mathbb{P}(A_i) \]
\end{enumerate}
The set $\Omega$ is called the \textbf{sample space}; the elements $\omega \in \Omega$ are called \textbf{outcomes};\footnote{The symbols $\Omega,\omega$ \inlatex{Omega,\textbackslash{}omega} are the upper- and lower-case forms, respectively, of the Greek letter \textit{omega}.} the subsets $A \subseteq \Omega$ are called \textbf{events}; and the function $\mathbb{P}$ is called the \textbf{probability measure}. Given an event $A$, the value $\mathbb{P}(A)$ is called the \textbf{probability of $A$}.
\end{definition}

There is a general notion of a probability space, which does not require the sample space $\Omega$ to be countable. This definition is significantly more technical%
%(\Cref{defProbabilitySpace})
, so we restrict our attention in this section to \textit{discrete} probability spaces. Thus, whenever we say `probability space' in this section, the probability space can be assumed to be discrete. However, when our proofs do not specifically use countability of $\Omega$, they typically are true of arbitrary probability spaces. As such, we will specify discreteness in the statement of results only when countability of the sample space is required.

\begin{example}
\label{exProbabilitySpaceSixSidedDie}
We model the roll of a fair six-sided die.

The possible \textbf{outcomes} of the roll are $1$, $2$, $3$, $4$, $5$ and $6$, so we can take $\Omega = [6]$ to be the sample space.

The \textbf{events} correspond with subsets of $[6]$. For example:
\begin{itemize}
\item $\{ 4 \}$ is the event that the die roll shows $4$. This event occurs with probability $\frac{1}{6}$.
\item $\{ 1, 3, 5 \}$ is the event that the die roll is odd. This event occurs with probability $\frac{1}{2}$
\item $\{ 1, 4, 6 \}$ is the event that the die roll is not prime. This event occurs with probability $\frac{1}{2}$.
\item $\{ 3, 4, 5, 6 \}$ is the event that the die roll shows a number greater than $2$. This event occurs with probability $\frac{2}{3}$.
\item $\{ 1, 2, 3, 4, 5, 6 \}$ is the event that anything happens. This event occurs with probability $1$.
\item $\varnothing$ is the event that nothing happens. This event occurs with probability $0$.
\end{itemize}

More generally, since each outcome occurs with equal probability $\frac{1}{6}$, we can define
\[ \mathbb{P}(A) = \frac{|A|}{6} \text{ for all events } A \]
We will verify that $\mathbb{P}$ defines a probability measure on $[6]$ in \Cref{exProbabilitySpaceSixSidedDieVerifyMeasure}.
\end{example}

\begin{example}
\label{exProbabilityOfEmptySet}
Let $(\Omega,\mathbb{P})$ be a probability space. We prove that $\mathbb{P}(\varnothing)=0$.

Note that $\Omega$ and $\varnothing$ are disjoint, so by countable additivity, we have
\[ 1 = \mathbb{P}(\Omega) = \mathbb{P}(\Omega \cup \varnothing) = \mathbb{P}(\Omega) + \mathbb{P}(\varnothing) = 1 + \mathbb{P}(\varnothing) \]
Subtracting $1$ throughout yields $\mathbb{P}(\varnothing) = 0$, as required.
\end{example}

\begin{exercise}
Let $(\Omega,\mathbb{P})$ be a probability space. Prove that
\[ \mathbb{P}(\Omega \setminus A) = 1-\mathbb{P}(A) \]
for all events $A$.
\end{exercise}

Countable additivity of probability measures---that is, condition (ii) in \Cref{defDiscreteProbabilitySpace}---implies that probabilities of events are determined by probabilities of individual outcomes. This is made precise in \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace}.

\begin{proposition}
\label{propEquivalentSecondAxiomsForDiscreteProbabilitySpace}
Let $\Omega$ be a countable set and let $\mathbb{P} : \mathcal{P}(\Omega) \to [0,1]$ be a function such that $\mathbb{P}(\Omega)=1$. The following are equivalent:
\begin{enumerate}[(i)]
\item $\mathbb{P}$ is a probability measure on $\Omega$;
\item $\sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \mathbb{P}(A)$ for all $A \subseteq \Omega$.
\end{enumerate}
\end{proposition}

\begin{cproof}
Since $\mathbb{P}(\Omega)=1$, it suffices to prove that condition (ii) of \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace} is equivalent to countable additivity of $\mathbb{P}$.

\begin{itemize}
\item (i)$\Rightarrow$(ii). Suppose $\mathbb{P}$ is a probability measure on $\Omega$. Let $A \subseteq \Omega$.

Note that since $A \subseteq \Omega$ and $\Omega$ is countably infinite, it follows that $\{ \{ \omega \} \mid \omega \in A \}$ is a countable family of pairwise disjoint sets. By countable additivity, we have
\[ \mathbb{P}(A) = \mathbb{P}\left( \bigcup_{\omega \in A} \{ \omega \} \right) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}) \]
as required. Hence condition (ii) of the proposition is satisfied.

\item (ii)$\Rightarrow$(i). Suppose that $\sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \mathbb{P}(A)$ for all $A \subseteq \Omega$. We prove that $\mathbb{P}$ is a probability measure on $\Omega$.

So let $\{ A_i \mid i \in I \}$ be a family of pairwise disjoint events, indexed by a countable set $I$. Define $A = \bigcup_{i \in I} A_i$. Since the sets $A_i$ partition $A$, summing over elements of $A$ is the same as summing over each of the sets $A_i$ individually, and then adding those results together; specifically, for each $A$-tuple $(p_{\omega})_{\omega \in A}$, we have
\[ \sum_{\omega \in A} p_{\omega} = \sum_{i \in I} \sum_{\omega \in A_i} p_{\omega} \]
Hence
\begin{align*}
\mathbb{P}(A) &= \sum_{\omega \in A} \mathbb{P}(\{\omega\}) && \text{by condition (ii) of the proposition} \\
&= \sum_{i \in I} \sum_{\omega \in A_i} \mathbb{P}(\{\omega\}) && \text{by the above observation} \\
&= \sum_{i \in I} \mathbb{P}(A_i) && \text{by condition (ii) of the proposition}
\end{align*}
So $\mathbb{P}$ satisfies the countable additivity condition. Thus $\mathbb{P}$ is a probability measure on $\Omega$.
\end{itemize}
Hence the two conditions are equivalent.
\end{cproof}

\begin{example}
\label{exProbabilitySpaceSixSidedDieVerifyMeasure}
We prove that the function $\mathbb{P}$ from \Cref{exProbabilitySpaceSixSidedDie} truly does define a probability measure. Indeed, let $\Omega=[6]$ and let $\mathbb{P} : \mathcal{P}(\Omega) \to [0,1]$ be defined by
\[ \mathbb{P}(A) = \frac{|A|}{6} \text{ for all events } A \]
Then $\mathbb{P}(\Omega) = \frac{6}{6} = 1$, so condition (i) in \Cref{defDiscreteProbabilitySpace} is satisfied. Moreover, for each $A \subseteq [6]$ we have
\[ \sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \sum_{\omega \in A} \frac{1}{6} = \frac{|A|}{6} = \mathbb{P}(A) \]
so, by \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace}, $\mathbb{P}$ defines a probability measure on $[6]$.
\end{example}

\Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace} makes defining probability measures much easier, since it implies that probability measures are determined entirely by their values on individual outcomes. This means that, in order to define a probability measure, we only need to specify its values on individual outcomes and check that the sum of these probabilities is equal to $1$. This is significantly easier than defining $\mathbb{P}(A)$ on \textit{all} events $A \subseteq \Omega$ and checking the two conditions of \Cref{defDiscreteProbabilitySpace}.

This is made precise in \Cref{propProbabilityMeasureDeterminedByIndividualOutcomes} below.

\begin{proposition}
\label{propProbabilityMeasureDeterminedByIndividualOutcomes}
Let $\Omega$ be a countable set and, for each $\omega \in \Omega$, let $p_{\omega} \in [0,1]$. If $\sum_{\omega \in \Omega} p_{\omega} = 1$, then there is a unique probability measure $\mathbb{P}$ on $\Omega$ such that $\mathbb{P}(\{ \omega \}) = p_{\omega}$ for each $\omega \in \Omega$.
\end{proposition}

\begin{cproof}
We prove existence and uniqueness of $\mathbb{P}$ separately.
\begin{itemize}
\item \textbf{Existence.} Define $\mathbb{P} : \mathcal{P}(\Omega) \to [0,1]$ be defined by
\[ \mathbb{P}(A) = \sum_{\omega \in A} p_{\omega} \]
for all events $A \subseteq \Omega$. Then condition (ii) of \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace} is automatically satisfied, and indeed $\mathbb{P}(\{\omega\}) = p_{\omega}$ for each $\omega \in \Omega$. Moreover
\[ \mathbb{P}(\Omega) = \sum_{\omega \in \Omega} \mathbb{P}(\{\omega\}) = \sum_{\omega \in \Omega} p_{\omega} = 1 \]
and so condition (i) of \Cref{defDiscreteProbabilitySpace} is satisfied. Hence $\mathbb{P}$ defines a probability measure on $\Omega$.
\item \textbf{Uniqueness.} Suppose that $\mathbb{P}' : \mathcal{P}(\Omega) \to [0,1]$ is another probability measure such that $\mathbb{P}'(\{\omega\})=p_{\omega}$ for all $\omega \in \Omega$. For each event $A \subseteq \Omega$, condition (ii) of \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace} implies that
\[ \mathbb{P}'(A) = \sum_{\omega \in A} \mathbb{P}'(\{\omega\}) = \sum_{\omega \in A} p_{\omega} = \mathbb{P}(A) \]
hence $\mathbb{P}'=\mathbb{P}$.
\end{itemize}
So $\mathbb{P}$ is uniquely determined by the values $p_{\omega}$.
\end{cproof}

The assignments of $p_{\omega} \in [0,1]$ to each $\omega \in \Omega$ in fact defines something that we will later defined to be a \textit{probability mass function} (\Cref{defPMF}).

With \Cref{propProbabilityMeasureDeterminedByIndividualOutcomes} proved, we will henceforth specify probability measures $\mathbb{P}$ on sample spaces $\Omega$ by specifying only the values of $\mathbb{P}(\{\omega\})$ for $\omega \in \Omega$.

\begin{example}
Let $p \in [0,1]$. A coin, which shows heads with probability $p$, is repeatedly flipped until heads shows.

The outcomes of such a sequence of coin flips all take the form
\[ (\underbrace{\text{tails}, \text{tails}, \cdots,\text{tails}}_{n \text{ `tails'}}, \text{heads}) \]
for some $n \in \mathbb{N}$. Identifying such a sequence with the number $n$ of flips before heads shows, we can take $\Omega = \mathbb{N}$ to be the sample space.

For each $n \in \mathbb{N}$, we can define
\[ \mathbb{P}(\{n\}) = (1-p)^np \]
This will define a probability measure on $\mathbb{N}$, provided these probabilities all sum to $1$; and indeed by \Cref{thmGeometricSeries}, we have
\[ \sum_{n \in \mathbb{N}} \mathbb{P}(\{n\}) = \sum_{n \in \mathbb{N}} (1-p)^np = p \cdot \frac{1}{1-(1-p)} = p \cdot \frac{1}{p} = 1 \]
By \Cref{propProbabilityMeasureDeterminedByIndividualOutcomes}, it follows that $(\Omega,\mathbb{P})$ is a probability space.
\end{example}

\begin{exercise}
A fair six-sided die is rolled twice. Define a probability space $(\Omega, \mathbb{P})$ that models this situation.
\end{exercise}

\begin{exercise}
\label{exProbabilityOfSubset}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events with $A \subseteq B$. Prove that $\mathbb{P}(A) \le \mathbb{P}(B)$.
\end{exercise}

\subsection*{Set operations on events}

In the real world, we might want to talk about the probability that two events both happen, or the probability that an event doesn't happen, or the probability that at least one of some collection of events happens. This is interpreted mathematically in terms of set operations.

\begin{example}
\label{exInformalOrAndNot}
Let $(\Omega,\mathbb{P})$ be the probability space modelling two rolls of a fair six-sided die---that is, the sample space $\Omega=[6] \times [6]$ with probability measure $\mathbb{P}$ defined by $\mathbb{P}(\{(a,b)\}) = \frac{1}{36}$ for each $(a,b) \in \Omega$.

Let $A$ be the event that the sum of the die rolls is even, that is
\[ A = \begin{Bmatrix}(1,1), & (1,3), & (1,5), & (2,2), & (2,4), & (2,6),\\
(3,1), & (3,3), & (3,5), & (4,2), & (4,4), & (4,6),\\
(5,1), & (5,3), & (5,5), & (6,2), & (6,4), & (6,6)\phantom{,}
\end{Bmatrix} \]
and let $B$ be the event that the sum of the die rolls is greater than or equal to $9$, that is
\[ B = \{ (3,6), (4,5), (4,6), (5,4), (5,5), (5,6), (6,3), (6,4), (6,5), (6,6) \} \]
Then
\begin{itemize}
\item Consider the event that the sum of the die rolls is even \textbf{or} greater than or equal to $9$. An outcome $\omega$ gives rise to this event precisely when either $\omega \in A$ or $\omega \in B$; so the event in question is $A \cup B$;
\item Consider the event that the sum of the die rolls is even \textbf{and} greater than or equal to $9$. An outcome $\omega$ gives rise to this event precisely when both $\omega \in A$ and $\omega \in B$; so the event in question is $A \cap B$;
\item Consider the event that the sum of the die rolls is \textbf{not} even. An outcome $\omega$ gives rise to this event precisely when $\omega \not \in A$; so the event in question is is $([6] \times [6]) \setminus A$.
\end{itemize}
Thus we can interpret `or' as union, `and' as intersection, and `not' as relative complement in the sample space.
\end{example}

The intuition provided by \Cref{exInformalOrAndNot} is formalised in \Cref{exFormalOrAndNot}. Before we do this, we adopt a convention that simplifies notation when discussing events in probability spaces.

\begin{definition}
\label{defComplementOfEvent}
\index{complement!of event}
\nindex{complement}{$A^c$}{complement of event}
Let $(\Omega,\mathbb{P})$ be a probability space. The \textbf{complement} of an event $A \subseteq \Omega$ is the event $\Omega \setminus A \subseteq \Omega$. We write $A^c$ \inlatexnb{A\^{}c} for $\Omega \setminus A$.
\end{definition}

That is, when we talk about the complement \textit{of an event}, we really mean their relative complement inside the sample space.

\begin{exercise}
\label{exFormalOrAndNot}
Let $(\Omega, \mathbb{P})$ be a probability space, and let $p(\omega),q(\omega)$ be logical formulae with free variable $\omega$ ranging over $\Omega$. Let
\[ A = \{ \omega \in \Omega \mid p(\omega) \} \quad \text{and} \quad B = \{ \omega \in \Omega \mid q(\omega) \} \]
Prove that
\begin{itemize}
\item $\{ \omega \in \Omega \mid p(\omega) \wedge q(\omega) \} = A \cap B$;
\item $\{ \omega \in \Omega \mid p(\omega) \vee q(\omega) \} = A \cup B$;
\item $\{ \omega \in \Omega \mid \neg p(\omega) \} = A^c$.
\end{itemize}
For reference, in \Cref{exInformalOrAndNot}, we had $\Omega = [6] \times [6]$ and we defined $p(a,b)$ to be `$a+b$ is even' and $q(a,b)$ to be `$a+b \ge 7$'.
\end{exercise}

With this in mind, it will be useful to know how set operations on events interact with probabilities. A useful tool in this investigation is that of an \textit{indicator function}.

\begin{definition}
\label{defIndicatorFunction}
\index{indicator function}
Let $\Omega$ be a set and let $A \subseteq \Omega$. The \textbf{indicator function} of $A$ in $\Omega$ is the function $i_A : \Omega \to \{ 0,1 \}$\nindex{iA}{$i_A$}{indicator function} defined by
\[ i_A(\omega) = \begin{cases} 1 & \text{if } \omega \in A \\ 0 & \text{if } \omega \not \in A \end{cases} \]
\end{definition}

\begin{proposition}
\label{propIndicatorFunctionSetOperations}
Let $\Omega$ be a set and let $A,B \subseteq \Omega$. Then for all $\omega \in \Omega$ we have
\begin{enumerate}[(i)]
\item $i_{A \cap B}(\omega) = i_A(\omega)i_B(\omega)$;
\item $i_{A \cup B}(\omega) = i_A(\omega) + i_B(\omega) - i_{A \cap B}(\omega)$; and
\item $i_{A^c}(\omega) = 1 - i_A(\omega)$.
\end{enumerate}
\end{proposition}

\begin{cproof}[of (i)]
Let $\omega \in \Omega$. If $\omega \in A \cap B$ then $\omega \in A$ and $\omega \in B$, so that $i_{A \cap B}(\omega) = i_A(\omega) = i_B(\omega) = 1$. Hence
\[ i_A(\omega)i_B(\omega) = 1 = i_{A \cap B}(\omega) \]
If $\omega \not \in A \cap B$ then either $\omega \not \in A$ or $\omega \not \in B$. Hence $i_{A \cap B}(\omega) = 0$, and either $i_A(\omega) = 0$ or $i_B(\omega) = 0$. Thus
\[ i_A(\omega) i_B(\omega) = 0 = i_{A \cap B}(\omega) \]
In both cases, we have $i_{A \cap B}(\omega) = i_A(\omega)i_B(\omega)$, as required.
\end{cproof}

\begin{exercise}
Prove parts (ii) and (iii) of \Cref{propIndicatorFunctionSetOperations}.
\end{exercise}

\begin{exercise}
\label{exProbabilityWithIndicatorFunction}
Let $(\Omega, \mathbb{P})$ be a discrete probability space, and for each $\omega \in \Omega$ let $p_{\omega} = \mathbb{P}(\{\omega\})$. Prove that, for each event $A$, we have
\[ \mathbb{P}(A) = \sum_{\omega \in \Omega} p_{\omega}i_A(\omega) \]
\end{exercise}

\begin{theorem}
\label{thmProbabilityOfUnion}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B \subseteq \Omega$. Then
\[ \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) \]
\end{theorem}

\begin{cproof}
For each $\omega \in \Omega$, let $p_{\omega} = \mathbb{P}(\{\omega\})$. Then
\begin{align*}
\mathbb{P}(A \cup B) &= \sum_{\omega \in \Omega} p_{\omega}i_{A \cup B}(\omega) && \text{by \Cref{exProbabilityWithIndicatorFunction}} \\
&= \sum_{\omega \in \Omega} p_{\omega}(i_A(\omega)+i_B(\omega)-i_{A \cap B}(\omega)) && \text{by \Cref{propIndicatorFunctionSetOperations}(ii)} \\
&= \sum_{\omega \in \Omega} p_{\omega}i_A(\omega) + \sum_{\omega \in \Omega} p_{\omega}i_B(\omega) + \sum_{\omega \in \Omega} p_{\omega}i_{A \cap B}(\omega) && \text{rearranging} \\
&= \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B) && \text{by \Cref{exProbabilityWithIndicatorFunction}}
\end{align*}
as required.
\end{cproof}

Although there are nice expressions for unions and complements of events, it is not always the case that intersection of events corresponds with multiplication of probabilities.

\begin{example}
Let $\Omega = [3]$ and define a probability measure $\mathbb{P}$ on $\Omega$ by letting
\[ \mathbb{P}(\{1\}) = \frac{1}{4}, \qquad \mathbb{P}(\{2\}) = \frac{1}{2} \qquad \text{and} \qquad \mathbb{P}(\{3\}) = \frac{1}{4} \]
Then we have
\[ \mathbb{P}(\{1,2\} \cap \{2,3\}) = \mathbb{P}(\{2\}) = \frac{1}{2} \ne \frac{9}{16} = \frac{3}{4} \cdot \frac{3}{4} = \mathbb{P}(\{1,2\}) \cdot \mathbb{P}(\{2,3\}) \]
\end{example}

This demonstrates that it is not always the case that $\mathbb{P}(A \cap B) = \mathbb{P}(A) \mathbb{P}(B)$ for events $A,B$ in a given probability space. Pairs of events $A,B$ for which this equation \textit{is} true are said to be \textit{independent}.

\begin{definition}
\label{defIndependentEvents}
\index{independent!events}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events. We say $A$ and $B$ are \textbf{independent} if $\mathbb{P}(A \cap B) = \mathbb{P}(A)\mathbb{P}(B)$; otherwise, we say they are \textbf{dependent}. More generally, events $A_1,A_2,\dots,A_n$ are \textbf{mutually independent} if
\[ \mathbb{P}(A_1 \cap A_2 \cap \cdots \cap A_n) = \mathbb{P}(A_1)\mathbb{P}(A_2) \cdots \mathbb{P}(A_n) \]
\end{definition}

\begin{example}
A fair six-sided die is rolled twice. Let $A$ be the event that the first roll shows $4$, and let $B$ be the event that the second roll is even. Then
\[ A = \{ (4,1), (4,2), (4,3), (4,4), (4,5), (4,6) \} \]
so $\mathbb{P}(A) = \frac{6}{36} = \frac{1}{6}$; and
\[ B = \{ (a,2), (a,4), (a,6) \mid a \in [6] \} \]
so $\mathbb{P}(B) = \frac{18}{36} = \frac{1}{2}$. Moreover $A \cap B = \{ (4,2), (4,4), (4,6) \}$, so it follows that
\[ \mathbb{P}(A \cap B) = \frac{3}{36} = \frac{1}{12} = \frac{1}{6} \cdot \frac{1}{2} = \mathbb{P}(A)\mathbb{P}(B) \]
so the events $A$ and $B$ are independent.

Let $C$ be the event that the sum of the two dice rolls is equal to $5$. Then
\[ C = \{ (1,4), (2,3), (3,2), (4,1) \} \]
so $\mathbb{P}(C) = \frac{4}{36} = \frac{1}{9}$. Moreover $A \cap C = \{ (4, 1) \}$, so it follows that
\[ \mathbb{P}(A \cap C) = \frac{1}{36} \ne \frac{1}{54} = \frac{1}{6} \cdot \frac{1}{9} = \mathbb{P}(A)\mathbb{P}(C) \]
so the events $A$ and $C$ are dependent.
\end{example}

\begin{exercise}
Let $(\Omega,\mathbb{P})$ be a probability space. Under what conditions is an event $A$ independent from itself?
\end{exercise}

\subsection*{Conditional probability}

Suppose we model a real-world situation, such as the roll of a die or the flip of a coin, using a probability $(\Omega,\mathbb{P})$. When we receive new information, the situation might change, and we might want to model this new situation by updating our probabilities to reflect the fact that we know that $B$ has occurred. This is done by defining a new probability measure $\widetilde{\mathbb{P}}$ on $\Omega$. What follows is an example of this.

\begin{example}
\label{exConditionalProbabilityWithCards}
Two cards are drawn at random, in order, without replacement, from a $52$-card deck. We can model this situation by letting the sample space $\Omega$ be the set of ordered pairs of distinct cards, and letting $\mathbb{P}$ assign an equal probability (of $\frac{1}{|\Omega|}$) to each outcome. Note that $|\Omega| = 52 \cdot 51$, and so
\[ \mathbb{P}(\{\omega\}) = \frac{1}{52 \cdot 51} \]
for each outcome $\omega$.

We will compute two probabilities:
\begin{itemize}
\item The probability that the second card drawn is a heart.
\item The probability that the second card drawn is a heart \textit{given that} the first card drawn is a diamond.
\end{itemize}

Let $A \subseteq \Omega$ be the event that the second card drawn is a heart, and let $B \subseteq \Omega$ be the event that the first card drawn is a diamond.

To compute $\mathbb{P}(A)$, note first that $A = A' \cup A''$, where
\begin{itemize}
\item $A'$ is the event that both cards are hearts, so that $|A'| = 13 \cdot 12$; and
\item $A''$ is the event that only the second card is a heart, so that $|A''| = 39 \cdot 13$.
\end{itemize}
Since $A' \cap A'' = \varnothing$, it follows from countable additivity that
\[ \mathbb{P}(A) = \mathbb{P}(A') + \mathbb{P}(A'') = \frac{13 \cdot 12 + 39 \cdot 13}{52 \cdot 51} = \frac{13 \cdot (12 + 39)}{52 \cdot 51} = \frac{1}{4} \]

Now suppose we know that first card drawn is a diamond---that is, event $B$ has occurred---and we wish to update our probability that $A$ occurs. We do this by defining a new probability measure
\[ \widetilde{\mathbb{P}} : \mathcal{P}(\Omega) \to [0,1] \]
such that:
\begin{enumerate}[(a)]
\item The outcomes that do not give rise to the event $B$ are assigned probability zero; that is, $\widetilde{\mathbb{P}}(\{\omega\}) = 0$ for all $\omega \not \in B$; and
\item The outcomes that give rise to the event $B$ are assigned probabilities proportional to their old probability; that is, there is some $k \in \mathbb{R}$ such that $\widetilde{\mathbb{P}}(\omega) = k\mathbb{P}(\omega)$ for all $\omega \in B$.
\end{enumerate}

In order for $\widetilde{\mathbb{P}}$ to be a probability measure on $\Omega$, we need condition (i) of \Cref{defDiscreteProbabilitySpace} to occur. 
\begin{align*}
\widetilde{\mathbb{P}}(\Omega) &= \sum_{\omega \in \Omega} \widetilde{\mathbb{P}}(\{\omega\}) && \text{by condition (ii) of \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace}} \\
&= \sum_{\omega \in B} \widetilde{\mathbb{P}}(\{\omega\}) && \text{since $\widetilde{\mathbb{P}}(\{\omega\})=0$ for $\omega \not \in B$} \\
&= \sum_{\omega \in B} k\mathbb{P}(\{\omega\}) && \text{since $\widetilde{\mathbb{P}}(\{\omega\})=k\mathbb{P}(\{\omega\}$ for $\omega \in B$} \\
&= k\mathbb{P}(B) && \text{by condition (ii) of \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace}}
\end{align*}
Since we need $\widetilde{\mathbb{P}}(\Omega)=1$, we must therefore take $k=\frac{1}{\mathbb{P}(B)}$. (In particular, we need $\mathbb{P}(B)>0$ for this notion to be well-defined.)

Recall that, before we knew that the first card was a diamond, the probability that the second card is a heart was $\frac{1}{4}$. We now calculate how this probability changes with the updated information that the first card was a diamond.

The event that the second card is a heart in the new probability space is precisely $A \cap B$, since it is the subset of $B$ consisting of all the outcomes $\omega$ giving rise to the event $A$. As such, the new probability that the second card is a heart is given by
\[ \widetilde{\mathbb{P}}(A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \]
Now:
\begin{itemize}
\item $A \cap B$ is the event that the first card is a diamond and the second is a heart. To specify such an event, we need only specify the ranks of the two cards, so $|A \cap B| = 13 \cdot 13$ and hence $\mathbb{P}(A \cap B) = \frac{13 \cdot 13}{52 \cdot 51}$.
\item $B$ is the event that the first card is a diamond. A similar procedure as with $A$ yields $\mathbb{P}(B) = \frac{1}{4}$. 
\end{itemize}
Hence
\[ \widetilde{\mathbb{P}}(A) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = \frac{13 \cdot 13 \cdot 4}{52 \cdot 51} = \frac{13}{51} \]
Thus the knowledge that the first card drawn is a diamond very slightly increases the probability that the second card is a heart from $\frac{1}{4}=\frac{13}{52}$ to $\frac{13}{51}$.
\end{example}

\Cref{exConditionalProbabilityWithCards} suggests the following schema: upon discovering that an event $B$ occurs, the probability that event $A$ occurs should change from $\mathbb{P}(A)$ to $\frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$. This motivates the following definition of \textit{conditional probability}.

\begin{definition}
\label{defConditionalProbability}
\index{conditional probability}
\index{probability!conditional}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events such that $\mathbb{P}(B)>0$. The \textbf{conditional probability of $A$ given $B$} is the number $\mathbb{P}(A \mid B)$\nindex{PrAB}{$\mathbb{P}(A \mid B)$}{conditional probability} \inlatex{mathbb\{P\}(A\ \textbackslash{}mid\ B)}\lindexmmc{mid}{$\mid$} defined by
\[ \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \]
\end{definition}

\begin{example}
A fair six-sided die is rolled twice. We compute the probability that the first roll showed a $2$ given that the sum of the die rolls is less than $5$.

We can model this situation by taking the sample space to be $[6] \times [6]$, with each outcome having an equal probability of $\frac{1}{36}$.

Let $A$ be the event that the first die roll shows a $2$, that is
\[ A = \{ (2,1),(2,2),(2,3),(2,4),(2,5),(2,6) \} \]
and let $B$ be the event that the sum of the die rolls is less than $5$, that is
\[ B = \{ (1,1),(1,2),(1,3),(2,1),(2,2),(3,1) \} \]
We need to compute $\mathbb{P}(A \mid B)$. Well,
\[ A \cap B = \{ (2,1), (2,2) \} \]
so $\mathbb{P}(A \cap B) = \frac{2}{36}$; and $\mathbb{P}(B) = \frac{6}{36}$. Hence
\[ \mathbb{P}(A \mid B) = \frac{\frac{2}{36}}{\frac{6}{36}} = \frac{2}{6} = \frac{1}{3} \]
\end{example}

\begin{exercise}
A fair six-sided die is rolled three times. What is the probability that the sum of the die rolls is less than or equal to $12$, given that each die roll shows a power of $2$?
\end{exercise}

\begin{exercise}
\label{exConditionalProbabilityOfIntersection}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events with $\mathbb{P}(B)>0$. Prove that
\[ \mathbb{P}(A \mid B) = \mathbb{P}(A \cap B \mid B) \]
\end{exercise}

\begin{exercise}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events such that $\mathbb{P}(B)>0$. Prove that $\mathbb{P}(A \mid B) = \mathbb{P}(A)$ if and only if $A$ and $B$ are independent.
\end{exercise}

We will soon see some useful real-world applications of probability theory using \textit{Bayes's theorem} (\Cref{thmBayes}). Before we do so, some technical results will be useful in our proofs.

\begin{proposition}
\label{propConditionalProbabilityPartition}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events with $0<\mathbb{P}(B)<1$. Then
\[ \mathbb{P}(A) = \mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c) \]
\end{proposition}

\begin{cproof}
Note first that we can write
\[ A = A \cap \Omega = A \cap (B \cup B^c) = (A \cap B) \cup (A \cap B^c) \]
and moreover the events $A \cap B$ and $A \cap B^c$ are mutually exclusive. Hence
\[ \mathbb{P}(A) = \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^c) \]
by countable additivity. The definition of conditional probability (\Cref{defConditionalProbability}) then gives
\[ \mathbb{P}(A) = \mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c) \]
as required.
\end{cproof}

\begin{example}
\label{exAnimalRescueCatsDogs}
An animal rescue centre houses a hundred animals, sixty of which are dogs and forty of which are cats. Ten of the dogs and ten of the cats hate humans. We compute the probability that a randomly selected animal hates humans.

Let $A$ be the event that a randomly selected animal hates humans, and let $B$ be the event that the animal is a dog. Note that $B^c$ is precisely the event that the animal is a cat. The information we are given says that:
\begin{itemize}
\item $\mathbb{P}(B) = \frac{60}{100}$, since $60$ of the $100$ animals are dogs;
\item $\mathbb{P}(B^c) = \frac{40}{100}$, since $40$ of the $100$ animals are cats;
\item $\mathbb{P}(A \mid B) = \frac{10}{60}$, since $10$ of the $60$ dogs hate humans;
\item $\mathbb{P}(A \mid B^c) = \frac{10}{40}$, since $10$ of the $40$ cats hate humans.
\end{itemize}
By \Cref{propConditionalProbabilityPartition}, it follows that the probability that a randomly selected animal hates humans is
\[ \mathbb{P}(A) = \mathbb{P}(A \mid B) \mathbb{P}(B) + \mathbb{P}(A \mid B^c) \mathbb{P}(B^c) = \frac{60}{100} \cdot \frac{10}{60} + \frac{40}{100} \cdot \frac{10}{40} = \frac{20}{100} = \frac{1}{5} \]
\end{example}

The following example generalises \Cref{propConditionalProbabilityPartition} to arbitrary partitions of a sample space into events with positive probabilities.

\begin{example}
\label{exAnimalRescueCatsDogsRabbits}
The animal rescue centre from \Cref{exAnimalRescueCatsDogs} acquires twenty additional rabbits, of whom sixteen hate humans. We compute the probability that a randomly selected animal hates humans, given the new arrivals.

A randomly selected animal must be either a dog, a cat or a rabbit, and each of these occurs with positive probability. Thus, letting $D$ be the event that the selected animal is a dog, $C$ be the event that the animal is a cat, and $R$ be the event that the animal is a rabbit, we see that the sets $D,C,R$ form a partition of the sample space.

Letting $A$ be the event that the selected animal hates humans. Then
\begin{align*}
\mathbb{P}(A) &= \mathbb{P}(A \mid D) \mathbb{P}(D) + \mathbb{P}(A \mid C) \mathbb{P}(C) + \mathbb{P}(A \mid R) \mathbb{P}(R) \\
&= \frac{10}{60} \cdot \frac{60}{120} + \frac{10}{40} \cdot \frac{40}{120} + \frac{16}{20} \cdot \frac{20}{120} \\
&= \frac{3}{10}
\end{align*}
\end{example}

\Cref{propConditionalProbabilityIsProbabilityMeasure} below is a technical result which proves that conditional probability truly does yield a new probability measure on a given sample space.

\begin{proposition}
\label{propConditionalProbabilityIsProbabilityMeasure}
Let $(\Omega,\mathbb{P})$ be a probability space and let $B$ be an event such that $\mathbb{P}(B)>0$. The function $\widetilde{\mathbb{P}} : \mathcal{P}(\Omega) \to [0,1]$ defined by
\[ \widetilde{\mathbb{P}}(A) = \mathbb{P}(A \mid B) \text{ for all } A \subseteq \Omega \]
defines a probability measure on $\Omega$.
\end{proposition}
\begin{cproof}
First note that
\[ \widetilde{\mathbb{P}}(\Omega) = \mathbb{P}(\Omega \mid B) = \frac{\mathbb{P}(\Omega \cap B)}{\mathbb{P}(B)} = \frac{\mathbb{P}(B)}{\mathbb{P}(B)} = 1 \]
so condition (i) of \Cref{defDiscreteProbabilitySpace} is satisfied.

Moreover, for each $A \subseteq \Omega$ we have
\begin{align*}
\widetilde{\mathbb{P}}(A) &= \mathbb{P}(A \mid B) && \text{by definition of $\widetilde{\mathbb{P}}$} \\
&= \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} && \text{by \Cref{defConditionalProbability}} \\
&= \frac{1}{\mathbb{P}(B)} \sum_{\omega \in A \cap B} \mathbb{P}(\{\omega\}) && \text{by \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace}} \\
&= \sum_{\omega \in A \cap B} \mathbb{P}(\{\omega\} \mid B) && \text{by \Cref{defConditionalProbability}} \\
&= \sum_{\omega \in A} \mathbb{P}(\{\omega\} \mid B) && \text{since $\mathbb{P}(\{\omega\} \mid B) = 0$ for $\omega \in A \setminus B$} \\
&= \sum_{\omega \in A} \widetilde{\mathbb{P}}(\{\omega\}) && \text{by definition of $\widetilde{\mathbb{P}}$}
\end{align*}
so condition (ii) of \Cref{propEquivalentSecondAxiomsForDiscreteProbabilitySpace} is satisfied. Hence $\widetilde{\mathbb{P}}$ defines a probability measure on $\Omega$.
\end{cproof}

\Cref{propConditionalProbabilityIsProbabilityMeasure} implies that we can use all the results we've proved about probability measures to conditional probability given a fixed event $B$. For example, \Cref{thmProbabilityOfUnion} implies that
\[ \mathbb{P}(A \cup A' \mid B) = \mathbb{P}(A \mid B) + \mathbb{P}(A' \mid B) - \mathbb{P}(A \cap A' \mid B) \]
for all events $A,A',B$ in a probability space $(\Omega,\mathbb{P})$ such that $\mathbb{P}(B)>0$.

The next theorem we prove has a very short proof, but is extremely important in applied probability theory.

\begin{theorem}[Bayes's theorem]
\label{thmBayes}
\index{Bayes's theorem}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events with positive probabilities. Then
\[ \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A)} \]
\end{theorem}

\begin{cproof}
\Cref{defConditionalProbability} gives
\[ \mathbb{P}(A \mid B)\mathbb{P}(B) = \mathbb{P}(A \cap B) = \mathbb{P}(B \cap A) = \mathbb{P}(B \mid A)\mathbb{P}(A) \]
Dividing through by $\mathbb{P}(A)$ yields the desired equation.
\end{cproof}

As stated, Bayes's theorem is not necessarily particularly enlightening, but its usefulness increases sharply when combined with \Cref{propConditionalProbabilityPartition} to express the denominator of the fraction in another way.

\begin{corollary}
\label{corBayes}
Let $(\Omega,\mathbb{P})$ be a probability space and let $A,B$ be events such that $\mathbb{P}(A)>0$ and $0<\mathbb{P}(B)<1$. Then
\[ \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B)\mathbb{P}(B)}{\mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c)} \]
\end{corollary}

\begin{cproof}
Bayes's theorem tells us that
\[ \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A)} \]
By \Cref{propConditionalProbabilityPartition} we have
\[ \mathbb{P}(A) = \mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c) \]
Substituting for $\mathbb{P}(A)$ therefore yields
\[ \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B) \mathbb{P}(B)}{\mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c)} \]
as required.
\end{cproof}

The following example is particularly counterintuitive.

\begin{example}
A town has $10000$ people, $30$ of whom are infected with Disease X. Medical scientists develop a test for Disease X, which is accurate $99\%$ of the time. A person takes the test, which comes back positive. We compute the probability that the person truly is infected with Disease X.

Let $A$ be the event that the person tests positive for Disease X, and let $B$ be the event that the person is infected with Disease X. We need to compute $\mathbb{P}(B \mid A)$.

By \Cref{corBayes}, we have
\[ \mathbb{P}(B \mid A) = \frac{\mathbb{P}(A \mid B)\mathbb{P}(B)}{\mathbb{P}(A \mid B)\mathbb{P}(B) + \mathbb{P}(A \mid B^c)\mathbb{P}(B^c)} \]
It remains to compute the individual probabilities on the right-hand side of this equation. Well,
\begin{itemize}
\item $\mathbb{P}(A \mid B)$ is the probability that the person tests positive for Disease X, given that they are infected. This is equal to $\frac{99}{100}$, since the test is accurate with probability $99\%$.
\item $\mathbb{P}(A \mid B^c)$ is the probability that the person tests positive for Disease X, given that they are \textit{not} infected. This is equal to $\frac{1}{100}$, since the test is \textit{inaccurate} with probability $1\%$.
\item $\mathbb{P}(B) = \frac{30}{10000}$, since $30$ of the $10000$ inhabitants are infected with Disease X.
\item $\mathbb{P}(B^c) = \frac{9970}{10000}$, since $9970$ of the $10000$ inhabitants are \textit{not} infected with Disease X.
\end{itemize}
Piecing this together gives
\[ \mathbb{P}(B \mid A) = \frac{\frac{99}{100} \cdot \frac{30}{10000}}{\frac{99}{100} \cdot \frac{30}{10000} + \frac{1}{100} \cdot \frac{9970}{10000}} = \frac{297}{1294} \approx 0.23 \]
Remarkably, the probability that the person is infected with Disease X given that the test is positive is only $23\%$, even though the test is accurate $99\%$ of the time!
\end{example}

The following result generalises \Cref{corBayes} to arbitrary partitions of the sample space into sets with positive probabilities.

\begin{corollary}
\label{corBayesGeneral}
Let $(\Omega,\mathbb{P})$ be a probability space, let $A$ be an event with $\mathbb{P}(A)>0$, and let $\{ B_i \mid i \in I \}$ be a family of mutually exclusive events indexed by a countable set $I$ such that
\[ \mathbb{P}(B_i) > 0 \text{ for all } i \in I \qquad \text{and} \qquad \bigcup_{i \in I} B_i = \Omega \]
Then
\[ \mathbb{P}(B_i \mid A) = \frac{\mathbb{P}(A \mid B_i) \mathbb{P}(B_i)}{\sum_{i \in I} \mathbb{P}(A \mid B_i)\mathbb{P}(B_i)} \]
for each $i \in I$.
\end{corollary}
\begin{cproof}
Bayes's theorem tells us that
\[ \mathbb{P}(B_i \mid A) = \frac{\mathbb{P}(A \mid B_i) \mathbb{P}(B_i)}{\mathbb{P}(A)} \]
By countable additivity, we have
\[ \mathbb{P}(A) = \mathbb{P}\left( \bigcup_{i \in I} A \cap B_i \right) = \sum_{i \in I} \mathbb{P}(A \cap B_i) = \sum_{i \in I} \mathbb{P}(A \mid B_i)\mathbb{P}(B_i) \]
Substituting for $\mathbb{P}(A)$ therefore yields
\[ \mathbb{P}(B_i \mid A) = \frac{\mathbb{P}(A \mid B_i) \mathbb{P}(B_i)}{\sum_{i \in I} \mathbb{P}(A \mid B_i)\mathbb{P}(B_i)} \]
as required.
\end{cproof}

\begin{example}
\label{exBayesCarCompany}
A small car manufacturer, \textit{Cars N'At}, makes three models of car: the \textit{Allegheny}, the \textit{Monongahela} and the \textit{Ohio}. It made $3000$ Alleghenys, $6500$ Monongahelas, and $500$ Ohios. In a given day, an Allegheny breaks down with probability $\frac{1}{100}$, a Monongahela breaks down with probability $\frac{1}{200}$, and the notoriously unreliable Ohio breaks down with probability $\frac{1}{20}$. An angry driver calls Cars N'At to complain that their car has broken down. We compute the probability that the driver was driving an Ohio.

Let $A$ be the event that the car is an Allegheny, let $B$ be the event that the car is a Monongahela, and let $C$ be the event that the car is an Ohio. Then
\[ \mathbb{P}(A) = \frac{3000}{10000}, \quad \mathbb{P}(B) = \frac{6500}{10000}, \quad \mathbb{P}(C) = \frac{500}{10000} \]
Let $D$ be the event that the car broke down. Then
\[ \mathbb{P}(D \mid A) = \frac{1}{100}, \quad \mathbb{P}(D \mid B) = \frac{1}{200}, \quad \mathbb{P}(D \mid C) = \frac{1}{20} \]
We need to compute $\mathbb{P}(C \mid D)$. Since the events $A,B,C$ partition the sample space and have positive probabilities, we can use \Cref{corBayesGeneral}, which tells us that
\[ \mathbb{P}(C \mid D) = \frac{\mathbb{P}(D \mid C)\mathbb{P}(C)}{\mathbb{P}(D \mid A)\mathbb{P}(A)+\mathbb{P}(D \mid B)\mathbb{P}(B)+\mathbb{P}(D \mid C)\mathbb{P}(C)} \]
Substituting the probabilities that we computed above, it follows that
\[ \mathbb{P}(C \mid D) = \frac{\frac{1}{20} \cdot \frac{500}{10000}}{\frac{1}{100} \cdot \frac{3000}{10000} + \frac{1}{200} \cdot \frac{6500}{10000} + \frac{1}{20} \cdot \frac{500}{10000}} = \frac{2}{7} \approx 0.29 \]
\end{example}

\begin{exercise}
In \Cref{exBayesCarCompany}, find the probabilities that the car was an Allegheny and that the car was a Monongahela.
\end{exercise}